{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "from meta_matching_tool import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to preprocess the data\n",
    "pos = pd.read_csv('positive.txt', sep='\\t')\n",
    "neg = pd.read_csv('negative.txt', sep='\\t')\n",
    "\n",
    "pos_adductlist = [\"M+H\",\"M+NH4\",\"M+Na\",\"M+ACN+H\",\"M+ACN+Na\",\"M+2ACN+H\",\"2M+H\",\"2M+Na\",\"2M+ACN+H\"]\n",
    "neg_adductlist = [\"M-H\", \"M-2H\", \"M-2H+Na\", \"M-2H+K\", \"M-2H+NH4\", \"M-H2O-H\", \"M-H+Cl\", \"M+Cl\", \"M+2Cl\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_metas = pd.read_csv('selected_feature_id.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Function for main model ######################\n",
    "\n",
    "def getLayerSizeList(partition, threshold_layer_size, sparsify_coefficient):\n",
    "    \"\"\"\n",
    "    Obtain the size of each sparse layer\n",
    "    \n",
    "    INPUT:\n",
    "    partition: the adjacent matrix of metabolic network\n",
    "    threshold_layer_size: the threshold of sparese layer\n",
    "    sparsify_coefficient: the coefficient of each sparse level\n",
    "    \n",
    "    OUTPUT:\n",
    "    sparsify_hidden_layer_size_dict: a dictionary indicating the sparse layer\n",
    "    \"\"\"\n",
    "    n_meta = np.shape(partition)[0]\n",
    "    n_layer = math.floor(np.log10(1.0 * threshold_layer_size / n_meta) / np.log10(sparsify_coefficient)) + 3\n",
    "    \n",
    "    # dict for number of neurons in each layer\n",
    "    sparsify_hidden_layer_size_dict = {}\n",
    "\n",
    "    sparsify_hidden_layer_size_dict['n_hidden_0'] = int(n_meta)\n",
    "\n",
    "    # How is this sparsing rate determined? TODO: check this\n",
    "    for i in range(1,n_layer):\n",
    "        sparsify_hidden_layer_size_dict['n_hidden_%d' % (i)] = int(n_meta * (sparsify_coefficient) ** (i-1))\n",
    "    return sparsify_hidden_layer_size_dict\n",
    "\n",
    "\n",
    "def getPartitionMatricesList(sparsify_hidden_layer_size_dict, degree_dict, feature_meta, partition):\n",
    "    \"\"\"\n",
    "    Obtain the linkage matrix among two sparse layers\n",
    "    \"\"\"\n",
    "    np.random.seed(1);  # for reproducable result\n",
    "    g = ig.Graph.Adjacency((partition).tolist(), mode = \"undirected\")\n",
    "    dist = np.array(g.shortest_paths()) # use the shortest distance matrix to assign links\n",
    "    \n",
    "    sum_remove_node_list = []  # keep note of which nodes are already removed\n",
    "    \n",
    "    partition_mtx_dict = {}\n",
    "    residual_connection_dic = {}\n",
    "\n",
    "    partition_mtx_dict[\"p0\"] = feature_meta  # first matrix being the connection from features to meta\n",
    "    partition_mtx_dict[\"p1\"] = partition  # first matrix being the whole adjacency matrix\n",
    "\n",
    "    # The code below adopted a seemingly very **stupid** way of determining the linkage. TODO: rewrite this\n",
    "    for i in range(2, len(sparsify_hidden_layer_size_dict)):\n",
    "        num_nodes_to_remove = sparsify_hidden_layer_size_dict[\"n_hidden_%d\" % (i-1)] - \\\n",
    "                              sparsify_hidden_layer_size_dict[\"n_hidden_%d\" % (i)]\n",
    "        # sort node degree dict according to number of degrees\n",
    "        sorted_node_degree_list = sorted(degree_dict.items(), key=lambda item: item[1])\n",
    "\n",
    "        # Directly take the position of the nodes that are needed to be removed.\n",
    "        temp_remove_list = []\n",
    "        max_to_remove_node_degree = sorted_node_degree_list[num_nodes_to_remove - 1][1]\n",
    "        \n",
    "        # any node with degree less than `max_to_remove_node_degree` is certain to be removed\n",
    "        for j in range(num_nodes_to_remove):  \n",
    "            if sorted_node_degree_list[j][1] < max_to_remove_node_degree:\n",
    "                id_to_remove_node = sorted_node_degree_list[j][0]\n",
    "                # print(sorted_node_degree_list[j])\n",
    "                temp_remove_list.append(id_to_remove_node)\n",
    "            else:\n",
    "                break  # node with more degrees is not under consideration\n",
    "        \n",
    "        # sample from all nodes that have max_to_remove_node_degree to reach number of nodes to remove\n",
    "        sample_list = []\n",
    "        for j in range(len(temp_remove_list), len(sorted_node_degree_list)):\n",
    "            if sorted_node_degree_list[j][1] == max_to_remove_node_degree:\n",
    "                sample_list.append(sorted_node_degree_list[j])\n",
    "            else:\n",
    "                break  # node with more degrees is not under consideration\n",
    "            \n",
    "        # Very interesting way of determining connection...\n",
    "        sample_idx_list = sorted(\n",
    "            np.random.choice(len(sample_list), num_nodes_to_remove - len(temp_remove_list), replace=False))\n",
    "        for idx in sample_idx_list:\n",
    "            temp_remove_list.append(sample_list[idx][0])\n",
    "\n",
    "        # sum up add nodes to be removed\n",
    "        all_list = np.arange(partition.shape[0])\n",
    "        previous_layer_list = [x for x in all_list if x not in sum_remove_node_list]\n",
    "        temp_partition = np.delete(partition, sum_remove_node_list, axis=0)\n",
    "        sum_remove_node_list += temp_remove_list\n",
    "        temp_partition = np.delete(temp_partition, sum_remove_node_list, axis=1)\n",
    "        next_layer_list = [x for x in all_list if x not in sum_remove_node_list]\n",
    "\n",
    "        # Residual connection layer\n",
    "        residual_location = [previous_layer_list.index(x) for x in next_layer_list]\n",
    "        \n",
    "        # assign each neuron at least one linkage\n",
    "        # I believe this is a mistake...\n",
    "        # for k in range(len(previous_layer_list)):\n",
    "        #     if sum(dist[k,next_layer_list]==float(\"inf\"))==len(next_layer_list):\n",
    "        #         idx = np.random.choice(len(next_layer_list), 1, replace=False)\n",
    "        #     else:\n",
    "        #         idx = np.argsort(dist[k,next_layer_list], axis = -1)[0]\n",
    "        #     temp_partition[k, idx] = 1\n",
    "            \n",
    "            \n",
    "        # Alternative version\n",
    "        for k in range(len(previous_layer_list)):\n",
    "            pos = previous_layer_list[k]\n",
    "            if sum(dist[pos,next_layer_list]==float(\"inf\"))==len(next_layer_list):\n",
    "                idx = np.random.choice(len(next_layer_list), 1, replace=False)\n",
    "            else:\n",
    "                idx = np.argsort(dist[pos,next_layer_list], axis = -1)[0]\n",
    "            temp_partition[k, idx] = 1\n",
    "        \n",
    "        for j in range(len(temp_remove_list)):\n",
    "            degree_dict.pop(temp_remove_list[j])\n",
    "            \n",
    "        # if i == len(sparsify_hidden_layer_size_dict) - 1:\n",
    "        #     print(next_layer_list)\n",
    "\n",
    "        partition_mtx_dict[\"p%d\" % i] = temp_partition\n",
    "\n",
    "        residual_connection_dic[\"p%d\" % i] = residual_location\n",
    "\n",
    "        print(residual_location)\n",
    "\n",
    "    return partition_mtx_dict, residual_connection_dic\n",
    "\n",
    "\n",
    "# This might not be used in my settings.\n",
    "def getNodeDegreeDict(partition):\n",
    "    \"\"\"\n",
    "    Obtain the node degree using the adjacent matrix of metabolic network\n",
    "    \"\"\"\n",
    "    degree_dict = {}\n",
    "    row, col = partition.shape\n",
    "    for i in range(row):\n",
    "        degree_dict[i] = -1  # decrease its own\n",
    "        for j in range(0, col):\n",
    "            if partition[i, j] == 1:\n",
    "                degree_dict[i] += 1\n",
    "\n",
    "    return degree_dict\n",
    "\n",
    "\n",
    "## Functions for backward selection.\n",
    "def getKeggidByIndex(raw_keggid, idxs, output_dir):\n",
    "    match_dic = {}\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# The logic here is disastrous... Maybe there is better implementation\n",
    "def backwardSelect(final_keggids, subgraph: ig.Graph, numberOfNodesList: list):\n",
    "    \n",
    "    indices = set([subgraph.vs.find(idx).index for idx in final_keggids])\n",
    "    # Keep track of all nodes in each layer for sparse connection\n",
    "    idsOfConnectedNodesEachLayer = [indices.copy()]\n",
    "\n",
    "    # Keep track of all nodes that have been connected\n",
    "    idxsHaveBeenConnected = indices.copy()\n",
    "\n",
    "    # number of output nodes must equal to number we pre-set\n",
    "    assert numberOfNodesList[-1] == len(final_keggids)\n",
    "\n",
    "    # Backward selection\n",
    "    numberOfNodesList.reverse()\n",
    "    numberOfNodesList.remove(len(final_keggids))\n",
    "    \n",
    "    for layerNumber, numberOfEachLayer in enumerate(numberOfNodesList):\n",
    "        # print(f\"At the beginning of {numberOfEachLayer}, the nodes are {idxsHaveBeenConnected}\")\n",
    "        currentNumber = len(idxsHaveBeenConnected)\n",
    "        numberOfNodesToBeConnected = numberOfEachLayer - currentNumber\n",
    "        \n",
    "        # The idxs to be newly connected in this layer\n",
    "        idxsToBeConnected = set()\n",
    "\n",
    "        # We only want those haven't been connected\n",
    "\n",
    "        idxsCanBeConnected = set(np.concatenate([subgraph.neighborhood(idx, order=1, mindist=1) for idx in idxsHaveBeenConnected]).astype(np.int32).flatten().tolist()) \\\n",
    "                            - idxsHaveBeenConnected\n",
    "        # print(idxsCanBeConnected)\n",
    "        \n",
    "        # print(f\"Current layer num: {numberOfEachLayer}, now we have {currentNumber} already.\")\n",
    "        # print(len(idxsCanBeConnected))\n",
    "\n",
    "        # if we happen to have more than we want to remove\n",
    "        if len(idxsCanBeConnected) >= numberOfNodesToBeConnected:\n",
    "            # print(f\"Current layer num: {numberOfEachLayer}, now we have {len(idxsCanBeConnected)} can be connected, and we before have {currentNumber}.\")\n",
    "            # print(sorted(idxsCanBeConnected))\n",
    "            idxsCanBeConnected= random.sample(sorted(idxsCanBeConnected), numberOfNodesToBeConnected)\n",
    "            idxsToBeConnected.update(idxsCanBeConnected)\n",
    "            idxsHaveBeenConnected.update(idxsCanBeConnected)\n",
    "            idsOfConnectedNodesEachLayer.append(sorted(idxsToBeConnected))\n",
    "            continue\n",
    "\n",
    "        # else we have less nodes\n",
    "        elif len(idxsCanBeConnected) < numberOfNodesToBeConnected:\n",
    "            # first we add them all\n",
    "            currentNumber += len(idxsCanBeConnected)\n",
    "            \n",
    "            idxsToBeConnected.update(idxsCanBeConnected)\n",
    "            idxsHaveBeenConnected.update(idxsCanBeConnected)\n",
    "            # print(f\"After extending one step, we have {currentNumber}, but we need {numberOfEachLayer}.\")\n",
    "            # if currentNumber > 800:\n",
    "            #     print(sorted(idxsHaveBeenConnected))\n",
    "\n",
    "            # print(currentNumber)\n",
    "\n",
    "            # while we don't have enough, we keep sampling until we have all we want\n",
    "            while currentNumber < numberOfEachLayer:\n",
    "                # if numberOfEachLayer == 913:\n",
    "                #     print(idxsHaveBeenConnected)\n",
    "                idxsCanBeConnected = set(np.concatenate([subgraph.neighborhood(idx, order=1, mindist=1) for idx in idxsHaveBeenConnected]).astype(np.int32).flatten().tolist()) \\\n",
    "                            - idxsHaveBeenConnected\n",
    "                # print(f\"Now we have {currentNumber}.\")\n",
    "                # print(f\"By cts extending one step, we have {len(idxsCanBeConnected)} can be added, and we need {numberOfEachLayer}\")\n",
    "                # print(currentNumber)\n",
    "                # If the connected subgraph is all selected, which is unlikely to happen, we randomly put needed node into the connection...\n",
    "                if len(idxsCanBeConnected) == 0:\n",
    "                    # print(f\"Random sampling happens in {numberOfEachLayer}, we sample {numberOfEachLayer - currentNumber} nodes.\")\n",
    "                    idxsCanBeConnected = set(range(subgraph.vcount())) - idxsHaveBeenConnected\n",
    "                    assert idxsCanBeConnected.isdisjoint(idxsHaveBeenConnected)\n",
    "                    # print((len(idxsHaveBeenConnected), numberOfNodesToBeConnected, len(idxsCanBeConnected)))\n",
    "                    idxsCanBeConnected = random.sample(sorted(idxsCanBeConnected), numberOfEachLayer - currentNumber)\n",
    "                    # print(sorted(idxsCanBeConnected))\n",
    "                    idxsToBeConnected.update(idxsCanBeConnected)\n",
    "                    idxsHaveBeenConnected.update(idxsCanBeConnected)\n",
    "                    break\n",
    "                \n",
    "                # If we still need more nodes, we just add them all\n",
    "                elif len(idxsCanBeConnected) < numberOfEachLayer - currentNumber:\n",
    "                    currentNumber += len(idxsCanBeConnected)\n",
    "                    idxsToBeConnected.update(idxsCanBeConnected)\n",
    "                    idxsHaveBeenConnected.update(idxsCanBeConnected)\n",
    "                    continue\n",
    "                \n",
    "                # When we have more nodes than we need\n",
    "                elif len(idxsCanBeConnected) >= numberOfEachLayer - currentNumber:\n",
    "                    idxsCanBeConnected = random.sample(sorted(idxsCanBeConnected), numberOfEachLayer - currentNumber)\n",
    "                    # print(numberOfNodesToBeConnected - currentNumber)\n",
    "                    idxsToBeConnected.update(idxsCanBeConnected)\n",
    "                    idxsHaveBeenConnected.update(idxsCanBeConnected)\n",
    "                    break\n",
    "                    \n",
    "            #when we have enough, then just go to another layer\n",
    "            idsOfConnectedNodesEachLayer.append(sorted(idxsToBeConnected))\n",
    "\n",
    "            # print(idxsToBeConnected)\n",
    "    mergedNodeList = [sorted(idsOfConnectedNodesEachLayer[0])]\n",
    "    for i in range(1, len(idsOfConnectedNodesEachLayer)):\n",
    "        # print(len(idsOfConnectedNodesEachLayer[i]))\n",
    "        temp = sorted(mergedNodeList[i-1] + list(idsOfConnectedNodesEachLayer[i]))\n",
    "        mergedNodeList.append(temp)\n",
    "        # print(len(temp))\n",
    "    mergedNodeList.reverse()\n",
    "    return mergedNodeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kegg = pd.read_csv(os.path.join(package_dir, 'data', 'kegg.txt'), sep='\\t')\n",
    "# # 步骤1: 提取唯一的 KEGGID-Name 映射（保留第一个出现的 Name）\n",
    "# unique_id_name = kegg[[\"KEGGID\", \"Name\"]].drop_duplicates(subset=\"KEGGID\", keep=\"first\")\n",
    "\n",
    "# # 步骤2: 将你的 KEGGID 数组转换为 DataFrame（假设你的数组名为 kegg_id_array）\n",
    "# kegg_id_array = metabolites # 替换为你的实际数组\n",
    "# query_df = pd.DataFrame({\"KEGGID\": kegg_id_array})\n",
    "\n",
    "# # 步骤3: 通过 merge 快速匹配名称（类似 SQL 的 JOIN）\n",
    "# result_df = query_df.merge(unique_id_name, on=\"KEGGID\", how=\"left\")\n",
    "\n",
    "# # 输出结果\n",
    "# print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLayerSizeList(partition, final_layer_size, sparsify_coefficient):\n",
    "    \"\"\"\n",
    "    Obtain the size of each sparse layer\n",
    "    \n",
    "    INPUT:\n",
    "    partition: the adjacent matrix of metabolic network\n",
    "    final_layer_size: the final of sparse layer\n",
    "    sparsify_coefficient: the coefficient of each sparse level\n",
    "    \n",
    "    OUTPUT:\n",
    "    sparsify_hidden_layer_size_dict: a dictionary indicating the sparse layer\n",
    "    \"\"\"\n",
    "    n_meta = np.shape(partition)[0]\n",
    "    n_layer = math.floor(np.log10(1.0 * final_layer_size / n_meta) / np.log10(sparsify_coefficient)) + 2\n",
    "    \n",
    "    # dict for number of neurons in each layer\n",
    "    sparsify_hidden_layer_size_dict = {}\n",
    "\n",
    "    sparsify_hidden_layer_size_dict['n_hidden_0'] = int(n_meta)\n",
    "\n",
    "    # How is this sparsing rate determined? TODO: check this\n",
    "    for i in range(1, n_layer):\n",
    "        sparsify_hidden_layer_size_dict['n_hidden_%d' % (i)] = int(final_layer_size / (sparsify_coefficient) ** (n_layer - i - 1))\n",
    "    return sparsify_hidden_layer_size_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of data: (1174, 704)\n",
      "The shape of feature-metabolites matching: (1174, 913)\n",
      "The shape of metabolic network: (913, 913)\n"
     ]
    }
   ],
   "source": [
    "# define a function to preprocess the data\n",
    "pos = pd.read_csv('positive.txt', sep='\\t')\n",
    "neg = pd.read_csv('negative.txt', sep='\\t')\n",
    "\n",
    "pos_adductlist = [\"M+H\",\"M+NH4\",\"M+Na\",\"M+ACN+H\",\"M+ACN+Na\",\"M+2ACN+H\",\"2M+H\",\"2M+Na\",\"2M+ACN+H\"]\n",
    "neg_adductlist = [\"M-H\", \"M-2H\", \"M-2H+Na\", \"M-2H+K\", \"M-2H+NH4\", \"M-H2O-H\", \"M-H+Cl\", \"M+Cl\", \"M+2Cl\"]\n",
    "\n",
    "data_annos, matchings, sub_graph,  metabolites, dic = data_preprocessing(pos=pos, neg=neg, idx_feature = 4, match_tol_ppm=5, zero_threshold=0.75, scale = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(625, 1174)\n",
      "(625,)\n"
     ]
    }
   ],
   "source": [
    "# Y information\n",
    "info = pd.read_csv(\"y.csv\")\n",
    "\n",
    "icu = (info['icu']).values\n",
    "cov = (info['cov']).values\n",
    "\n",
    "print()\n",
    "idx_cov = np.where((info['cov'] == 'Yes'))[0]  #& (info['day']=='d0')\n",
    "# HUGE mistake!!!!\n",
    "expression = (data_annos.iloc[:,idx_cov + 4].T).values\n",
    "\n",
    "print(expression.shape)\n",
    "\n",
    "y = np.zeros(len(idx_cov))\n",
    "y[np.where(icu[idx_cov] == 'Yes')] = 1  # change between cov/icu\n",
    "target = y.astype(int)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(625,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pos.result.20', 'pos.result.23', 'pos.result.24', 'pos.result.27',\n",
       "       'pos.result.39', 'pos.result.47', 'pos.result.53', 'pos.result.62',\n",
       "       'pos.result.63', 'pos.result.114',\n",
       "       ...\n",
       "       'neg.result.2398', 'neg.result.2454', 'neg.result.2462',\n",
       "       'neg.result.2495', 'neg.result.2544', 'neg.result.2654',\n",
       "       'neg.result.2774', 'neg.result.2786', 'neg.result.2800',\n",
       "       'neg.result.2812'],\n",
       "      dtype='object', length=1174)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_annos.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(data: pd.DataFrame, indices, p_value = 0.05):\n",
    "\n",
    "    \"\"\"A simple function for variable selection.\n",
    "    INPUT: \n",
    "    data: a feature by sample data frame\n",
    "    indices: a tuple indicating the indices of the samples of different labels, i.e. ([T],[F])\n",
    "    number: number of features we want\n",
    "    p_value: threshold for the two sample t test \n",
    "    \"\"\"\n",
    "    group_1 = data.iloc[:, indices[0]]\n",
    "    group_2 = data.iloc[:, indices[1]]\n",
    "    # 对每个特征进行t检验，并获取结果\n",
    "    results = data.apply(\n",
    "        lambda row: stats.ttest_ind(a = group_1.loc[row.name], b = group_2.loc[row.name]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 转换为包含统计量和p值的DataFrame\n",
    "    results_df = pd.DataFrame(results.tolist(), columns=['t_statistic', 'p_value'], index=data.index)\n",
    "\n",
    "    # 按p值升序排序\n",
    "    results_df.sort_values('p_value', inplace=True)\n",
    "\n",
    "    # 重置索引，将特征名作为单独的一列（可选）\n",
    "    results_df.reset_index(inplace=True)\n",
    "\n",
    "    print(results_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_5th = data_annos.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                index  t_statistic       p_value\n",
      "0     neg.result.1190    10.641740  2.053710e-24\n",
      "1     pos.result.3601     9.476665  5.393706e-20\n",
      "2     pos.result.1546     9.288549  2.576048e-19\n",
      "3      neg.result.898     8.702359  2.896983e-17\n",
      "4     pos.result.1708     8.475659  1.690401e-16\n",
      "...               ...          ...           ...\n",
      "1169   pos.result.954    -0.008117  9.935262e-01\n",
      "1170  pos.result.1470    -0.004710  9.962433e-01\n",
      "1171   neg.result.288     0.004550  9.963709e-01\n",
      "1172   pos.result.290    -0.002384  9.980988e-01\n",
      "1173   pos.result.625     0.000390  9.996887e-01\n",
      "\n",
      "[1174 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "res = featureSelection(data_annos.iloc[:, 4 + idx_cov], (y == 1, y == 0), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_metas = [dic[key] for key in res['index'].values[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_meta = np.concatenate(key_metas).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C00822',\n",
       " 'C05560',\n",
       " 'C01179',\n",
       " 'C01197',\n",
       " 'C05350',\n",
       " 'C00257',\n",
       " 'C00800',\n",
       " 'C16356',\n",
       " 'C16360',\n",
       " 'C00352',\n",
       " 'C06377',\n",
       " 'C12214']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phynn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
