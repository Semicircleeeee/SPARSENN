{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from model_function import *\n",
    "from random import seed\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "random.seed(222)\n",
    "\n",
    "# Get the current file's absolute path\n",
    "current_file_path = os.path.abspath('mouse_main.ipynb')\n",
    "\n",
    "# Get the parent directory\n",
    "parent_directory = os.path.join(os.path.dirname(os.path.dirname(current_file_path)), 'real_data_preprocessing/Mouse_brain/processed')\n",
    "\n",
    "save_path = os.path.join(os.path.dirname(os.path.dirname(current_file_path)), 'notebooks/res')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "\n",
    "data = pd.read_csv(os.path.join(parent_directory,\"data.csv\"))\n",
    "sample_id = data.columns.values[5:data.shape[1]]\n",
    "sample_id[455] = '498307_CSH_LA_M6_PO'\n",
    "X = (data.iloc[:,5:data.shape[1]]).values\n",
    "X = X.T\n",
    "\n",
    "m_min = np.min(X, 0)\n",
    "m_max = np.max(X, 0)\n",
    "\n",
    "X = np.log(1+X)\n",
    "\n",
    "expression = ((X - m_min)/(m_max - m_min)-0.5) * 100\n",
    "\n",
    "feature_meta = pd.read_csv(os.path.join(parent_directory,\"feature_meta_matching.csv\"))\n",
    "\n",
    "data = pd.read_csv(os.path.join(parent_directory,\"data.csv\"))\n",
    "mode = ['pos_' if 'pos' in i else 'neg_' for i in data['Unnamed: 0']]\n",
    "feature_name = [mode[i]+str(round(data['mz'][i],3)) for i in range(len(data['mz']))]\n",
    "\n",
    "c_name = (feature_meta.columns).values[1:]\n",
    "feature_meta = feature_meta.values[:,1:]\n",
    "feature_meta.shape\n",
    "\n",
    "# compound adj matrix\n",
    "partition = (pd.read_csv(os.path.join(parent_directory,\"adj_mtx.csv\"))).values[:,1:]\n",
    "partition.shape\n",
    "\n",
    "# assign the diag to be 1\n",
    "np.fill_diagonal(partition, 1)\n",
    "\n",
    "sample_id = np.array([i.replace('.mzML','') for i in sample_id])\n",
    "sample_id = np.array([i.replace('_02','') for i in sample_id])\n",
    "sample_id = np.array([i.replace('_002','') for i in sample_id])\n",
    "area = np.array([i[17:19] for i in sample_id])\n",
    "age = np.array([i[11:13] for i in sample_id])\n",
    "import collections\n",
    "area_summary = collections.Counter(area)\n",
    "\n",
    "age_summary = collections.Counter(age)\n",
    "print(age_summary)\n",
    "\n",
    "choose_idx = np.arange(len(age))\n",
    "choose_age = age[choose_idx]\n",
    "\n",
    "no_area = 'AD'\n",
    "\n",
    "drop_idx = np.where(choose_age == no_area)[0]\n",
    "choose_age = np.delete(choose_age, drop_idx)\n",
    "choose_idx = np.delete(choose_idx, drop_idx)\n",
    "\n",
    "expression = X[choose_idx,:]\n",
    "\n",
    "y = np.zeros([len(choose_idx)])\n",
    "\n",
    "ion_matrix = pd.read_csv(os.path.join(parent_directory,\"ion_matching.csv\"))\n",
    "ion_matrix = ion_matrix.values[:,1:]\n",
    "\n",
    "# AD young; EA adult; LA elder;\n",
    "\n",
    "y[np.where(choose_age=='LA')] = 1\n",
    "target = y.astype(int)\n",
    "\n",
    "brain_list = np.unique(area)\n",
    "brain_idx = np.array([np.where(brain_list == i)[0][0] for i in area])\n",
    "area_matrix = np.zeros([len(area), 10])\n",
    "area_matrix[np.arange(len(area)), brain_idx] = 1\n",
    "area_matrix = area_matrix[choose_idx,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val, area_train, area_val = train_test_split(\n",
    "         expression, target, area_matrix, test_size = 0.2,random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsify_coefficient = 0.3  #0.2\n",
    "threshold_layer_size = 100\n",
    "num_hidden_layer_neuron_list = [20]  # if assigning a null list. it means directly predicting (sparsifying results to two-neuron output layer) (default)\n",
    "sparsify_hidden_layer_size_dict = getLayerSizeList(partition, threshold_layer_size, sparsify_coefficient)\n",
    "degree_dict = getNodeDegreeDict(partition)\n",
    "partition_mtx_dict = getPartitionMatricesList(sparsify_hidden_layer_size_dict, degree_dict, feature_meta, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,partition_mtx_dict, num_hidden_layer_neuron_list, keep_prob):\n",
    "        super(Net,self).__init__()\n",
    "        layer1 = nn.Sequential()\n",
    "        for i in range(len(partition_mtx_dict)):\n",
    "            mtx = partition_mtx_dict[\"p%d\" % i]  # the mask matrix\n",
    "            layer1.add_module('f'+ str(i), SparseLinear(mtx.shape[0], mtx.shape[1], mtx))\n",
    "            layer1.add_module(\"f_relu\"+str(i), nn.ReLU(True))\n",
    "            layer1.add_module(\"bn1\"+str(i), nn.BatchNorm1d(mtx.shape[1]))\n",
    "        self.layer1 = layer1\n",
    "        \n",
    "        layer2 = nn.Sequential()\n",
    "        num_hidden_layer_neuron_list = [mtx.shape[1] + 10] + num_hidden_layer_neuron_list + [2]  # add 10 to identify different area\n",
    "        for j in range(1, len(num_hidden_layer_neuron_list)-1):\n",
    "            layer2.add_module('h'+str(j), myLinear(num_hidden_layer_neuron_list[j-1], num_hidden_layer_neuron_list[j]))\n",
    "            layer2.add_module('h_relu'+str(j), nn.ReLU(True))\n",
    "            layer2.add_module(\"bn2\"+str(j), nn.BatchNorm1d(num_hidden_layer_neuron_list[j]))\n",
    "            layer2.add_module('h_drop'+str(j), nn.Dropout(p=keep_prob))\n",
    "        layer2.add_module('h'+str(j+1), myLinear(num_hidden_layer_neuron_list[j], num_hidden_layer_neuron_list[j+1]))\n",
    "        self.layer2 = layer2\n",
    "        \n",
    "    def forward(self,input,area_info):\n",
    "        out = self.layer1(input)\n",
    "        out = torch.cat((out, area_info),-1)  # here we consider ten different brain area, thus use 10 more nodes to represent various areas\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "s = [3,9,11,12,15,18,27,28,30,37] # new seed\n",
    "\n",
    "for i in range(10):\n",
    "    file_name = os.path.join(save_path,\"mouse_\"+str(i)+'_.pkl')\n",
    "\n",
    "    seed = s[i]\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    keep_prob = 0.3\n",
    "    net = Net(partition_mtx_dict, num_hidden_layer_neuron_list, keep_prob)\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.1, weight_decay = 0.001)  # # lr = 0.1 # wd = 0.1\n",
    "\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    x = (torch.from_numpy(x_train)).type(torch.FloatTensor)\n",
    "    y = (torch.from_numpy(y_train)).type(torch.LongTensor)\n",
    "    z = (torch.from_numpy(area_train)).type(torch.FloatTensor)\n",
    "\n",
    "    torch_dataset = Data.TensorDataset(x, y, z)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset,     \n",
    "        batch_size=BATCH_SIZE,     \n",
    "        shuffle=True,              \n",
    "        num_workers=0           \n",
    "    )\n",
    "\n",
    "    x = Variable(x)\n",
    "    y = Variable(y)\n",
    "    z = Variable(z)\n",
    "\n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "    acc_val_i = 0\n",
    "    accuracy = 0\n",
    "    best_test = 0\n",
    "    corr_train = 0\n",
    "    for epoch in range(200): \n",
    "        if ((acc_val_i >0.9) &(accuracy>0.9)):\n",
    "            break\n",
    "        net.train()\n",
    "        if epoch<20:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.97\n",
    "        if epoch >20:\n",
    "            optimizer.param_groups[0]['lr'] *= 0.9\n",
    "\n",
    "        for step, (x_batch, y_batch, z_batch) in enumerate(loader): \n",
    "            x_batch = x_batch\n",
    "            y_batch = y_batch\n",
    "            z_batch = z_batch\n",
    "            optimizer.zero_grad()\n",
    "            prediction = net(x_batch, z_batch)\n",
    "            loss = loss_func(prediction,y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            prediction = net(x,z)\n",
    "            pred_y = prediction.data.numpy().squeeze()\n",
    "            target_y = y.cpu().data.numpy()\n",
    "            \n",
    "            accuracy = sum(target_y ==np.argmax(pred_y, axis=1))/len(target_y)  # 预测中有多少和真实值一样\n",
    "            acc_train.append(accuracy)\n",
    "\n",
    "            val_input_tensor = (torch.from_numpy(x_val)).type(torch.FloatTensor)\n",
    "            val_input_tensor_z = (torch.from_numpy(area_val)).type(torch.FloatTensor)\n",
    "            out_probs = net(val_input_tensor, val_input_tensor_z).data.numpy().squeeze()\n",
    "            out_classes = np.argmax(out_probs, axis=1)\n",
    "            acc_val_i = sum(out_classes == y_val) / len(y_val)\n",
    "            acc_val.append(sum(out_classes == y_val) / len(y_val))\n",
    "            if acc_val_i > best_test:\n",
    "                best_test = acc_val_i\n",
    "                corr_train = accuracy\n",
    "                torch.save(net, file_name) \n",
    "\n",
    "    plt.plot(acc_val, color = \"green\", label = \"validation\")\n",
    "    plt.plot(acc_train, color = \"red\", label = \"train\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    file_name =os.path.join(save_path,\"mouse_accuracy.csv\")\n",
    "    if not(os.path.exists(file_name)):\n",
    "        with open(file_name,\"a\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "    with open((file_name),\"a\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list([corr_train, best_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    net = torch.load(os.path.join(save_path,\"mouse_\"+str(i)+'_.pkl'))\n",
    "    params = []\n",
    "    for parameters in net.parameters():\n",
    "        params.append(parameters.cpu().detach().numpy())\n",
    "\n",
    "    h0 = params[0].T\n",
    "    h1 = params[4].T\n",
    "    h2 = params[8].T\n",
    "\n",
    "    meta_left = abs(h1).sum(axis = 0)\n",
    "    meta_right = abs(h2).sum(axis = 1)\n",
    "\n",
    "    meta_imp = meta_right/meta_right.sum() + meta_left/meta_left.sum()\n",
    "\n",
    "    degree = np.sum(partition,0)+1\n",
    "    count_feature = np.sum(feature_meta, 0)+1\n",
    "    f_left = np.sum(abs(h0),0)/(count_feature**0.5)\n",
    "    f_right = np.sum(abs(h1),1)/(degree**0.5)\n",
    "    f_imp = (f_left/f_left.sum()) + (f_right/f_right.sum())\n",
    "\n",
    "    dat = {'imp': list(f_imp),\n",
    "          'name': list(c_name)}\n",
    "    df = pd.DataFrame(dat)\n",
    "\n",
    "    n_feature = len(feature_name)\n",
    "\n",
    "    max_idx = np.argmax(abs(h0) / f_imp / meta_imp, axis=1)\n",
    "    Link = abs(h0)/f_imp/meta_imp\n",
    "\n",
    "    h0_new = np.zeros(h0.shape)\n",
    "    h0_new[range(n_feature), max_idx] = h0[range(n_feature), max_idx]\n",
    "    h0_new[h0_new!=0] = 1\n",
    "    feature_imp = np.dot(h0_new, f_imp)\n",
    "\n",
    "    file_name =os.path.join(save_path, \"mouse_meta_imp.csv\")\n",
    "    if not(os.path.exists(file_name)):\n",
    "        with open(file_name,\"a\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "    with open((file_name),\"a\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list(f_imp))\n",
    "\n",
    "    file_name = os.path.join(save_path,\"mouse_feature_imp.csv\")\n",
    "    if not(os.path.exists(file_name)):\n",
    "        with open(file_name,\"a\") as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "    with open((file_name),\"a\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(list(feature_imp))\n",
    "\n",
    "    file_name = os.path.join(save_path,\"mouse_link.txt\")\n",
    "    if not(os.path.exists(file_name)):\n",
    "        np.savetxt(os.path.join(save_path,\"mouse_link.txt\"),Link)\n",
    "    else:\n",
    "        old=np.loadtxt(os.path.join(save_path,\"mouse_link.txt\"), delimiter=' ')\n",
    "        new = old + Link\n",
    "        np.savetxt(os.path.join(save_path,\"mouse_link.txt\"),Link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join(save_path,\"mouse_meta_imp.csv\")\n",
    "meta_total = pd.read_csv(file_name,header = None)\n",
    "meta_avg = np.mean(meta_total.values, axis = 0)\n",
    "f_imp = meta_avg\n",
    "\n",
    "file_name = os.path.join(save_path,\"mouse_feature_imp.csv\")\n",
    "feature_total = pd.read_csv(file_name,header = None)\n",
    "feature_avg = np.mean(feature_total.values, axis = 0)\n",
    "feature_imp = feature_avg\n",
    "\n",
    "Link = np.loadtxt(os.path.join(save_path,\"mouse_link.txt\"), delimiter=' ')\n",
    "\n",
    "dat = {'imp': list(f_imp),\n",
    "      'name': list(c_name)}\n",
    "df = pd.DataFrame(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null % 0.75 ; "
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects\n",
    "import rpy2.robjects.numpy2ri\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "ro.r.assign(\"x\", f_imp)\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "robjects.r(\n",
    "           '''\n",
    "            fdrgamma<-function(x, use.percentile=NA, min.use.percentile=0.5)\n",
    "            {\n",
    "             library(MASS)\n",
    "\n",
    "             x0<-x\n",
    "             if(length(x)>50000) x<-sample(x, 50000, replace=F)\n",
    "\n",
    "             all.use.percentile<-use.percentile\n",
    "             if(is.na(use.percentile)) all.use.percentile<-seq(min.use.percentile,0.99,by=0.01)\n",
    "\n",
    "             d.x<-density(x,from=0)\n",
    "             x.mode<-d.x$x[which(d.x$y==max(d.x$y))[1]]\n",
    "             rec<-rep(0, length(all.use.percentile))\n",
    "\n",
    "             for(i in 1:length(all.use.percentile))\n",
    "             {\n",
    "              use.percentile<-all.use.percentile[i]\n",
    "              params<-fitdistr(x[x<quantile(x,use.percentile)],\"gamma\")\n",
    "\n",
    "              d.x.lim<-density(x,from=0,to=quantile(x,use.percentile))\n",
    "              d.null.lim<-dgamma(d.x.lim$x, shape=params$estimate[1], rate=params$estimate[2])*use.percentile\n",
    "              sel<-which(d.x.lim$x <= x.mode)\n",
    "              rec[i]<-sum((d.x.lim$y[sel]-d.null.lim[sel])^2)\n",
    "\n",
    "             }\n",
    "\n",
    "             sel<-which(rec==min(rec))[1]\n",
    "             use.percentile=all.use.percentile[sel]\n",
    "             cat('null %', use.percentile, \"; \")\n",
    "\n",
    "             d.x<-density(x,from=0, n=min(512, length(x)/2))\n",
    "             params<-fitdistr(x[x<quantile(x,use.percentile)],\"gamma\")\n",
    "             d.null<-dgamma(d.x$x, shape=params$estimate[1], rate=params$estimate[2])\n",
    "\n",
    "             null.mode.x<-d.x$x[which(d.null==max(d.null))[1]]\n",
    "\n",
    "             #plot(d.x, type=\"l\")\n",
    "             hist(x,nclass=min(round(length(x)/10),100),freq=FALSE)\n",
    "             lines(d.x$x, d.null*use.percentile,col=\"red\")\n",
    "             lines(d.x, col=\"blue\")\n",
    "\n",
    "             lfdr<-d.null/d.x$y*use.percentile\n",
    "             lfdr[lfdr>1]<-1\n",
    "             lfdr[is.na(lfdr)]<-1\n",
    "             lfdr[d.x$x<=null.mode.x]<-1\n",
    "             sel<-which(lfdr==max(lfdr))[1]\n",
    "             lfdr[1:sel]<-max(lfdr)\n",
    "             lfdr<-cummin(lfdr)\n",
    "\n",
    "             all.lfdr<-approx(d.x$x, lfdr, x0, rule=2)\n",
    "\n",
    "             #cut<-min(which(d.x$x>median(x) & lfdr<=fdr.cut))\n",
    "             #xcut<-d.x$x[cut]\n",
    "             #if(is.na(xcut)) xcut<-Inf\n",
    "             #abline(v=xcut, col=\"blue\")\n",
    "             return(all.lfdr[[2]])\n",
    "            }\n",
    "            \n",
    "            res = fdrgamma(x, use.percentile=0.75)\n",
    "           '''\n",
    "           )\n",
    "fdr = robjects.r['res']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = sum(fdr<0.05)\n",
    "print('The number of selected metbaolites %d' %thres)\n",
    "\n",
    "sel_idx = np.argsort(-f_imp)[0:thres]\n",
    "sel_name = c_name[sel_idx]\n",
    "sel_name\n",
    "\n",
    "import igraph\n",
    "\n",
    "np.fill_diagonal(partition, 0) # change the diag of partition\n",
    "\n",
    "g_sub = igraph.Graph.Adjacency((partition[sel_idx,:][:,sel_idx] > 0).tolist(), mode = \"undirected\")\n",
    "\n",
    "g_sub.vs[\"size\"] = 33\n",
    "g_sub.vs['label'] = sel_name\n",
    "g_sub.vs[\"label_size\"] = 8\n",
    "\n",
    "palette = igraph.drawing.colors.AdvancedGradientPalette(['#ff964f','#ffe5ad'], n=len(sel_idx))\n",
    "\n",
    "g_sub.vs[\"color\"]= palette\n",
    "g_sub.es['color'] = \"darkgrey\"\n",
    "g_sub.es['size'] = 0.1\n",
    "g_sub.es['width'] = 2\n",
    "\n",
    "igraph.plot(g_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# use part of r code ###########\n",
    "import rpy2.robjects as ro\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects.lib.dplyr import DataFrame\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "ro.r.assign(\"imp\", df.iloc[:,0].values)\n",
    "ro.r.assign(\"name\", df.iloc[:,1].values)\n",
    "ro.r.assign(\"thres\", thres)\n",
    "\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "robjects.r(\n",
    "           '''\n",
    "            library(metapone)\n",
    "            data(pa)\n",
    "            idx <- order(imp,decreasing=TRUE)[1:thres]\n",
    "\n",
    "            pathway_name_100 <- c()\n",
    "            for (i in 1:thres) {\n",
    "              p <- pa[which(pa$KEGG.ID==name[idx[i]]),2]\n",
    "              pathway_name_100 <- c(pathway_name_100, p)\n",
    "            }\n",
    "\n",
    "            meta_name_sub = name\n",
    "            meta_name_100 = name[idx]\n",
    "\n",
    "            # pathway related to the top 100 compound\n",
    "            pathway_name_100 = unique(pathway_name_100)\n",
    "\n",
    "            df = as.data.frame(matrix(nrow=0,ncol=5))\n",
    "            for (n in pathway_name_100) {\n",
    "              c = pa[which(pa$pathway.name==n),4]\n",
    "              in_100 = sum(c %in% meta_name_100)\n",
    "              in_sub = sum(c %in% meta_name_sub)\n",
    "              in_100_name = paste(c[c %in% meta_name_100],collapse=\",\")\n",
    "              in_sub_name = paste(c[c %in% meta_name_sub],collapse=\",\")\n",
    "              p_value = 1.0-phyper(in_100-1, thres, length(imp)-thres, in_sub)\n",
    "              df <- rbind(df, c(n, in_100_name, in_sub_name, p_value, paste(c[c %in% meta_name_100],collapse=\",\")))\n",
    "            }\n",
    "\n",
    "            names(df) <- c(\"pathway\",  \"num_in_selected\", \"num_in_all\", \"p_value\", \"compound id\")\n",
    "           '''\n",
    "           )\n",
    "\n",
    "res = robjects.r['df']\n",
    "\n",
    "go_res = pd.DataFrame(res)\n",
    "go_res['p_value'] = (go_res['p_value']).astype('float')\n",
    "go_res.sort_values(\"p_value\",inplace=True)\n",
    "go_res.reset_index(drop=True,inplace = True)\n",
    "go_res.to_csv(os.path.join(save_path,'mouse_go_restult.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phynn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
